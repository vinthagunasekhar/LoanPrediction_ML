{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-10T05:20:41.892983Z",
     "start_time": "2024-12-10T05:20:41.816752Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                           roc_curve, precision_recall_curve, auc, average_precision_score,\n",
    "                           confusion_matrix, classification_report)\n",
    "from sklearn.calibration import calibration_curve\n",
    "import os\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "\n",
    "# Set visualization style for consistent, professional-looking plots\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "\n",
    "class LoanModelEvaluator:\n",
    "    \"\"\"\n",
    "    A comprehensive evaluation framework that combines technical metrics with business insights\n",
    "    for loan prediction models.\n",
    "    \"\"\"\n",
    "    def __init__(self, models_dict, train_data, test_data, output_dir=None):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator with models and data.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        models_dict : dict\n",
    "            Dictionary containing trained models from Phase 2\n",
    "        train_data : pandas.DataFrame\n",
    "            Processed training data\n",
    "        test_data : pandas.DataFrame\n",
    "            Raw test data that needs processing\n",
    "        output_dir : str, optional\n",
    "            Directory to save evaluation results\n",
    "        \"\"\"\n",
    "        self.models = models_dict\n",
    "        self.train_data = train_data\n",
    "        self.test_data = self._process_test_data(test_data, train_data)\n",
    "\n",
    "        # Create results directory\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.results_dir = output_dir or f\"evaluation_results_{self.timestamp}\"\n",
    "        os.makedirs(self.results_dir, exist_ok=True)\n",
    "\n",
    "        # Business parameters for financial analysis\n",
    "        self.params = {\n",
    "            'default_loss_rate': 0.15,  # Expected loss on defaulted loans\n",
    "            'profit_margin': 0.10,      # Expected profit on good loans\n",
    "            'processing_cost': 1000      # Cost per application\n",
    "        }\n",
    "\n",
    "    def _process_test_data(self, test_df, train_df):\n",
    "        \"\"\"\n",
    "        Process test data to match training data format and features.\n",
    "        \"\"\"\n",
    "        test_processed = test_df.copy()\n",
    "\n",
    "        # Handle missing values using training data statistics\n",
    "        numeric_cols = ['LoanAmount', 'Loan_Amount_Term', 'Credit_History']\n",
    "        for col in numeric_cols:\n",
    "            test_processed[col].fillna(train_df[col].median(), inplace=True)\n",
    "\n",
    "        categorical_cols = ['Gender', 'Married', 'Dependents', 'Self_Employed']\n",
    "        for col in categorical_cols:\n",
    "            test_processed[col].fillna(train_df[col].mode()[0], inplace=True)\n",
    "\n",
    "        # Create engineered features\n",
    "        test_processed['Total_Income'] = (test_processed['ApplicantIncome'] +\n",
    "                                        test_processed['CoapplicantIncome'])\n",
    "\n",
    "        # Log transformations\n",
    "        for col in ['Total_Income', 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']:\n",
    "            test_processed[f'Log_{col}'] = np.log(test_processed[col] + 1)\n",
    "\n",
    "        # Financial ratios\n",
    "        test_processed['Income_to_Loan_Ratio'] = test_processed['Total_Income'] / test_processed['LoanAmount']\n",
    "        test_processed['EMI'] = (test_processed['LoanAmount'] * 1000 * 0.1) / 12\n",
    "        test_processed['Balance_Income'] = test_processed['Total_Income'] - test_processed['EMI']\n",
    "\n",
    "        # Handle categorical variables\n",
    "        test_processed = pd.get_dummies(test_processed,\n",
    "                                      columns=['Education', 'Property_Area'],\n",
    "                                      drop_first=True)\n",
    "\n",
    "        # Ensure column consistency with training data\n",
    "        train_cols = train_df.drop('Loan_Status', axis=1).columns\n",
    "        for col in train_cols:\n",
    "            if col not in test_processed.columns:\n",
    "                test_processed[col] = 0\n",
    "\n",
    "        return test_processed[train_cols]\n",
    "\n",
    "    def _calculate_business_metrics(self, y_true, y_pred, loan_amounts):\n",
    "        \"\"\"\n",
    "        Calculate business-oriented metrics including financial impact.\n",
    "        \"\"\"\n",
    "        # Identify different prediction cases\n",
    "        true_positives = (y_pred == 1) & (y_true == 1)\n",
    "        false_positives = (y_pred == 1) & (y_true == 0)\n",
    "        false_negatives = (y_pred == 0) & (y_true == 1)\n",
    "\n",
    "        # Calculate financial impacts\n",
    "        potential_losses = (loan_amounts[false_positives] * self.params['default_loss_rate']).sum()\n",
    "        missed_opportunities = (loan_amounts[false_negatives] * self.params['profit_margin']).sum()\n",
    "        expected_profits = (loan_amounts[true_positives] * self.params['profit_margin']).sum()\n",
    "        processing_costs = len(y_true) * self.params['processing_cost']\n",
    "\n",
    "        return {\n",
    "            'potential_losses': potential_losses,\n",
    "            'missed_opportunities': missed_opportunities,\n",
    "            'expected_profits': expected_profits,\n",
    "            'processing_costs': processing_costs,\n",
    "            'net_impact': expected_profits - potential_losses -\n",
    "                         missed_opportunities - processing_costs\n",
    "        }\n",
    "\n",
    "    def evaluate_model_performance(self, model, name, data='train'):\n",
    "        \"\"\"\n",
    "        Comprehensive evaluation of a single model's performance.\n",
    "        \"\"\"\n",
    "        # Select appropriate dataset\n",
    "        if data == 'train':\n",
    "            X = self.train_data.drop('Loan_Status', axis=1)\n",
    "            y = self.train_data['Loan_Status']\n",
    "        else:\n",
    "            X = self.test_data\n",
    "            y_pred = model.predict(X)\n",
    "            return {'predictions': y_pred,\n",
    "                   'probabilities': model.predict_proba(X)[:, 1]}\n",
    "\n",
    "        # Get predictions\n",
    "        y_pred = model.predict(X)\n",
    "        y_prob = model.predict_proba(X)[:, 1]\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y, y_pred),\n",
    "            'precision': precision_score(y, y_pred),\n",
    "            'recall': recall_score(y, y_pred),\n",
    "            'f1_score': f1_score(y, y_pred),\n",
    "            'roc_auc': auc(*roc_curve(y, y_prob)[:2]),\n",
    "            'avg_precision': average_precision_score(y, y_prob)\n",
    "        }\n",
    "\n",
    "        # Calculate business metrics\n",
    "        business_metrics = self._calculate_business_metrics(\n",
    "            y, y_pred, X['LoanAmount'] * 1000)\n",
    "\n",
    "        # Create visualizations\n",
    "        self._plot_model_diagnostics(y, y_pred, y_prob, name, data)\n",
    "\n",
    "        return {**metrics, **business_metrics}\n",
    "\n",
    "    def _plot_model_diagnostics(self, y_true, y_pred, y_prob, model_name, dataset):\n",
    "        \"\"\"\n",
    "        Create and save diagnostic plots for model performance.\n",
    "        \"\"\"\n",
    "        # Confusion Matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=['Not Approved', 'Approved'],\n",
    "                   yticklabels=['Not Approved', 'Approved'])\n",
    "        plt.title(f'Confusion Matrix - {model_name} ({dataset})')\n",
    "        plt.savefig(os.path.join(self.results_dir,\n",
    "                                f'confusion_matrix_{model_name}_{dataset}.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # ROC Curve\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "        plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc(fpr, tpr):.3f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curve - {model_name} ({dataset})')\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(self.results_dir,\n",
    "                                f'roc_curve_{model_name}_{dataset}.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # Calibration Curve\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=10)\n",
    "        plt.plot(prob_pred, prob_true, marker='o')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('Mean Predicted Probability')\n",
    "        plt.ylabel('True Probability')\n",
    "        plt.title(f'Calibration Curve - {model_name} ({dataset})')\n",
    "        plt.savefig(os.path.join(self.results_dir,\n",
    "                                f'calibration_curve_{model_name}_{dataset}.png'))\n",
    "        plt.close()\n",
    "\n",
    "    def analyze_feature_importance(self, model, name):\n",
    "        \"\"\"\n",
    "        Analyze and visualize feature importance.\n",
    "        \"\"\"\n",
    "        importance = pd.DataFrame({\n",
    "            'feature': self.train_data.drop('Loan_Status', axis=1).columns,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(data=importance, x='importance', y='feature')\n",
    "        plt.title(f'Feature Importance - {name}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.results_dir, f'feature_importance_{name}.png'))\n",
    "        plt.close()\n",
    "\n",
    "        return importance\n",
    "\n",
    "    def generate_comprehensive_report(self):\n",
    "        \"\"\"\n",
    "        Generate a comprehensive evaluation report for all models.\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'train_performance': {},\n",
    "            'test_predictions': {},\n",
    "            'feature_importance': {}\n",
    "        }\n",
    "\n",
    "        for name, model in self.models.items():\n",
    "            # Evaluate on training data\n",
    "            print(f\"\\nEvaluating {name}...\")\n",
    "            results['train_performance'][name] = self.evaluate_model_performance(\n",
    "                model, name, 'train')\n",
    "\n",
    "            # Generate test predictions\n",
    "            results['test_predictions'][name] = self.evaluate_model_performance(\n",
    "                model, name, 'test')\n",
    "\n",
    "            # Analyze feature importance\n",
    "            results['feature_importance'][name] = self.analyze_feature_importance(\n",
    "                model, name)\n",
    "\n",
    "        # Save results\n",
    "        self._save_results(results)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _save_results(self, results):\n",
    "        \"\"\"\n",
    "        Save evaluation results to files.\n",
    "        \"\"\"\n",
    "        # Save training performance metrics\n",
    "        pd.DataFrame(results['train_performance']).to_csv(\n",
    "            os.path.join(self.results_dir, 'training_performance.csv'))\n",
    "\n",
    "        # Save test predictions\n",
    "        for name, preds in results['test_predictions'].items():\n",
    "            pd.DataFrame({\n",
    "                'Predicted_Label': preds['predictions'],\n",
    "                'Probability': preds['probabilities']\n",
    "            }).to_csv(os.path.join(self.results_dir, f'test_predictions_{name}.csv'))\n",
    "\n",
    "        # Save feature importance\n",
    "        for name, importance in results['feature_importance'].items():\n",
    "            importance.to_csv(os.path.join(self.results_dir,\n",
    "                                         f'feature_importance_{name}.csv'))\n",
    "\n",
    "# Example usage:\n",
    "# Assuming we have our models from Phase 2\n",
    "model_dict = {\n",
    "    'Decision Tree': dt_grid.best_estimator_,\n",
    "    'Random Forest': rf_grid.best_estimator_,\n",
    "    'Gradient Boosting': gb_grid.best_estimator_\n",
    "}\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv('processed_train_data_final.csv')\n",
    "test_data = pd.read_csv('Test_Data.csv')\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = LoanModelEvaluator(model_dict, train_data, test_data)\n",
    "\n",
    "# Generate comprehensive evaluation\n",
    "results = evaluator.generate_comprehensive_report()\n",
    "\n",
    "# Display summary of results\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "performance_df = pd.DataFrame(results['train_performance']).round(3)\n",
    "print(performance_df)\n",
    "\n",
    "print(\"\\nTest Predictions Summary:\")\n",
    "for name in model_dict.keys():\n",
    "    preds = results['test_predictions'][name]['predictions']\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(\"Prediction Distribution:\")\n",
    "    print(pd.Series(preds).value_counts(normalize=True).round(3))\n",
    "\n"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dt_grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[31], line 265\u001B[0m\n\u001B[1;32m    259\u001B[0m             importance\u001B[38;5;241m.\u001B[39mto_csv(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresults_dir,\n\u001B[1;32m    260\u001B[0m                                          \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeature_importance_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.csv\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m    262\u001B[0m \u001B[38;5;66;03m# Example usage:\u001B[39;00m\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# Assuming we have our models from Phase 2\u001B[39;00m\n\u001B[1;32m    264\u001B[0m model_dict \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m--> 265\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDecision Tree\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[43mdt_grid\u001B[49m\u001B[38;5;241m.\u001B[39mbest_estimator_,\n\u001B[1;32m    266\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRandom Forest\u001B[39m\u001B[38;5;124m'\u001B[39m: rf_grid\u001B[38;5;241m.\u001B[39mbest_estimator_,\n\u001B[1;32m    267\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mGradient Boosting\u001B[39m\u001B[38;5;124m'\u001B[39m: gb_grid\u001B[38;5;241m.\u001B[39mbest_estimator_\n\u001B[1;32m    268\u001B[0m }\n\u001B[1;32m    270\u001B[0m \u001B[38;5;66;03m# Load data\u001B[39;00m\n\u001B[1;32m    271\u001B[0m train_data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprocessed_train_data_final.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'dt_grid' is not defined"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a0479b7abaa9accf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
